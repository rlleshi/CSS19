{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import timeit\n",
    "import requests\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_year(year, yearContent, df):\n",
    "    \"\"\" Crawl the different years of the wikipedia's archieved deletion discussions page and store the content\n",
    "        in a Data Frame. \n",
    "\n",
    "        Args: \n",
    "            year: the year in which the archived articles were flagged for deletion\n",
    "            yearContent: the html content containing the year (a h2 tag)\n",
    "            df: the data frame where the data are stored in the format year | month | title | Id | Gender\n",
    "\n",
    "        Returns:\n",
    "            a data frame\n",
    "    \"\"\"\n",
    "    for monthContent in yearContent.find_next_siblings(limit=24):\n",
    "        if monthContent.name == \"h2\":\n",
    "            # Crawl only this year. If the year doesn't yet have 12 months(e.g. 2019), don't go for more.\n",
    "            break\n",
    "        elif monthContent.name == \"h3\":\n",
    "            month = monthContent.get_text().split(str(year)+\" \")[1].split(\"[\")[0]\n",
    "            print(\"Month\",month)\n",
    "        elif monthContent.name == \"ul\":\n",
    "            # Go through the list of days\n",
    "            for dayRelative in monthContent.find_all(\"a\"):\n",
    "                print(dayRelative['href'])\n",
    "                dayPageLink = \"https://en.wikipedia.org/\"+dayRelative['href']\n",
    "                try :\n",
    "                    dayPage = requests.get(dayPageLink)\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    continue\n",
    "                soupPage = BeautifulSoup(dayPage.content, \"html.parser\")\n",
    "\n",
    "                if dayPage.status_code == 200:\n",
    "                    \n",
    "                    # Get the number of articles in a particular day\n",
    "                    # From the beginning till the june 2006 wikipedia has a different HTML code on this\n",
    "                    if (int(year) < 2006 | ((int(year) == 2006) & (month in ['June', 'May', 'April', 'March', 'February', \n",
    "                                                                          'January']))):\n",
    "                        try:\n",
    "                            articlesLength = float(soupPage.find_all(\"li\", {\"class\": \"toclevel-2\"})[-1].get_text().split(\" \")[0])\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        nrLength = len(str(articlesLength).split(\".\")[1])\n",
    "                        if nrLength == 2:\n",
    "                            articlesLength = round(articlesLength%1 * 100,2)\n",
    "                        elif nrLength == 3:\n",
    "                            articlesLength = round(articlesLength%1 * 1000,3)\n",
    "                    else:\n",
    "                        try:\n",
    "                            articlesLength = float(soupPage.find_all(\"ul\")[2].find_all(\"li\")[-1].get_text().split(\" \")[0])\n",
    "                        except ValueError:\n",
    "                            try:\n",
    "                                articlesLength = float(soupPage.find_all(\"ul\")[0].find_all(\"li\")[-1].get_text().split(\" \")[0])\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                        numberDec = round(articlesLength % 1 * 10, 2)\n",
    "                        if int(numberDec) != numberDec:\n",
    "                            numberDec *= 10\n",
    "                        articlesLength = int(articlesLength) + numberDec\n",
    "                    \n",
    "                    print(\"Articles to be crawled in this page: \",articlesLength)\n",
    "\n",
    "                    # Every article is located in an <h3> tag\n",
    "                    for article in soupPage.find_all(\"h3\", limit = articlesLength):\n",
    "                        try:\n",
    "                            # Don't read deleted articles\n",
    "                            if article.find(\"a\")['title'].find(\"(page does\") == -1:\n",
    "                                articleTitle = article.get_text().split(\"[\")[0]\n",
    "                                pageLink = \"https://en.wikipedia.org\"+article.find(\"a\")['href']\n",
    "                                df = crawl_article(year, month, articleTitle, pageLink, df)\n",
    "                        except Exception as e:\n",
    "                            continue\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_article(year, month, title, pageLink, df):\n",
    "    \"\"\" Crawl the content of the corresponding dbpedia page of a wikipedia article in order to get its id and gender.\n",
    "        Store an entry in the dataframe.\n",
    "\n",
    "        Args:\n",
    "            year: the year in which the current article was flagged for deletion\n",
    "            month: the month in which the current article was flagged for deletion\n",
    "            articleTitle: the title of the article flagged for deletion\n",
    "            pageLink: the wikipedia link of the article\n",
    "            df: the data frame where the data are stored in the format year | month | title | Id | gender\n",
    "\n",
    "        Returns:\n",
    "            A data frame\n",
    "    \"\"\"\n",
    "    url = \"http://dbpedia.org/page/\"+pageLink.split(\"/wiki/\")[1]\n",
    "    try :\n",
    "        dbpediaPage = requests.get(url)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return df\n",
    "    soup = BeautifulSoup(dbpediaPage.content, \"html.parser\")\n",
    "    wikiIdTag = soup.find(\"span\", {\"property\":\"dbo:wikiPageID\"})\n",
    "    genderTag = soup.find(\"span\", {\"property\":\"foaf:gender\"})\n",
    "    if genderTag == None:\n",
    "        # Not a person\n",
    "        return df\n",
    "    dic = {\"Year\":year, \"Month\":month, \"Tile\":title, \"Id\": wikiIdTag.contents[0]\n",
    "           , \"Gender\":genderTag.contents[0]}\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(data=dic, index=[0])\n",
    "    else:\n",
    "        df_temp = pd.DataFrame(data=dic, index=[0])\n",
    "        df = pd.concat([df, df_temp])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month December\n",
      "/wiki/Wikipedia:Articles_for_deletion/Log/2006_December_31\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8f22d4c2f050>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# print(years[10])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawl_year\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myears\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myearContents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0melapsedTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-739833419ae9>\u001b[0m in \u001b[0;36mcrawl_year\u001b[1;34m(year, yearContent, df)\u001b[0m\n\u001b[0;32m     33\u001b[0m                     \u001b[1;31m# Get the number of articles in a particular day\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                     \u001b[1;31m# From the beginning till the june 2006 wikipedia has a different HTML code on this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                     if (int(year) < 2006 | (int(year) == 2006 & month in ['June', 'May', 'April', 'March', 'February', \n\u001b[0m\u001b[0;32m     36\u001b[0m                                                                           'January'])):\n\u001b[0;32m     37\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "startTime = timeit.default_timer()\n",
    "\n",
    "seedURL = \"https://en.wikipedia.org/wiki/Wikipedia:Archived_deletion_discussions#Deletion_discussions/\"\n",
    "archivePage = requests.get(seedURL)\n",
    "soup = BeautifulSoup(archivePage.content, \"html.parser\")\n",
    "\n",
    "# Get the year\n",
    "years = []\n",
    "yearContents = []\n",
    "for yearContent in soup.find_all(\"h2\", limit=17):\n",
    "    year = yearContent.get_text().split(\"[\")[0]\n",
    "    if year == \"Contents\":\n",
    "        continue\n",
    "    years.append(year)\n",
    "    yearContents.append(yearContent)\n",
    "\n",
    "# print(years[10])\n",
    "df = pd.DataFrame()\n",
    "df = crawl_year(years[13], yearContents[13], df)\n",
    "\n",
    "elapsedTime = timeit.default_timer() - startTime\n",
    "print(\"Crawl time \", elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = df.to_csv (r'C:\\Users\\neiral\\WS_semester2\\CSS\\nominatedForDeletion\\2014.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('2013.csv')\n",
    "df = pd.read_csv('2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2592\n",
      "3231\n"
     ]
    }
   ],
   "source": [
    "# print(df2)\n",
    "print(len(df2))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195.00000000000006\n",
      "195.0\n",
      "195.0\n",
      "Articles to be crawled in this page:  196.0\n"
     ]
    }
   ],
   "source": [
    "articlesLength = 1.195\n",
    "print(articlesLength%1*1000)\n",
    "numberDec = round(articlesLength % 1 * 1000, 3)\n",
    "print(numberDec)\n",
    "if numberDec >= 10 & isinstance(numberDec, int):\n",
    "    temp = numberDec / 10\n",
    "    if isinstance(temp, int):\n",
    "        numberDec = temp\n",
    "else:\n",
    "    while isinstance(numberDec, float):\n",
    "        numberDec *= 10\n",
    "print(numberDec)\n",
    "articlesLength = int(articlesLength) + numberDec\n",
    "print(\"Articles to be crawled in this page: \",articlesLength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190.0\n"
     ]
    }
   ],
   "source": [
    "articlesLength = 1.19\n",
    "articlesLength = round(articlesLength%1 * 1000,3)\n",
    "print(articlesLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193.0\n"
     ]
    }
   ],
   "source": [
    "articlesLength = 1.193\n",
    "nrLength = len(str(articlesLength).split(\".\")[1])\n",
    "if nrLength == 2:\n",
    "    articlesLength = round(articlesLength%1 * 100,2)\n",
    "elif nrLength == 3:\n",
    "    articlesLength = round(articlesLength%1 * 1000,3)\n",
    "print(articlesLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Css - binder",
   "language": "python",
   "name": "cssresearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
